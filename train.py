# -*- coding: utf-8 -*-
"""YOLOv8n-MoE Training Script with Custom Training Loop

NOTE: This script currently trains a baseline YOLOv8n model as a fallback.
For proper MoE training with FIXED metrics calculation (no doubled precision/recall),
use train_moe_custom.py instead, which includes:
- Single-pass forward computation to prevent doubled metrics
- Load balancing loss for expert utilization
- Proper expert usage tracking

See METRICS_FIX.md for more details.
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from ultralytics import YOLO
from ultralytics.nn.modules import Conv, C2f, SPPF, Detect
from ultralytics.data import build_dataloader, build_yolo_dataset
from ultralytics.utils import LOGGER, colorstr
from torch.nn import Upsample
import math
import os
import yaml
from pathlib import Path
from tqdm import tqdm

# --- Configuration ---
DATA_YAML_PATH = './pet-dataset-yolo/pet_moe.yaml'  # Path to dataset YAML configuration
CHECKPOINT_DIR = './checkpoints'  # Directory to save checkpoints

# Read number of classes from dataset configuration
def get_num_classes():
    """Read number of classes from dataset YAML file."""
    if os.path.exists(DATA_YAML_PATH):
        with open(DATA_YAML_PATH, 'r') as f:
            data_config = yaml.safe_load(f)
            return data_config.get('nc', 37)  # Default to 37 if not found
    return 37  # Default fallback

NUM_CLASSES = get_num_classes()

# Training hyperparameters
EPOCHS = 100
BATCH_SIZE = 16  # Reduced for M1 Pro memory
IMG_SIZE = 640
LEARNING_RATE = 0.001
WEIGHT_DECAY = 0.0005

# --- Model Parameters (YOLOv8n Scale & MoE) ---
d = 0.33  # Depth multiplier
w = 0.25  # Width multiplier
nc = NUM_CLASSES
E = 4     # Total Experts
K = 2     # Top-K experts to select

def make_divisible(x, divisor=8):
    """Make channels divisible by divisor."""
    return math.ceil(x * w / divisor) * divisor

# Channel dimensions
C1 = make_divisible(64)   # 16
C2 = make_divisible(128)  # 32
C3 = make_divisible(256)  # 64
C4 = make_divisible(512)  # 128
C5 = make_divisible(1024) # 256
N3 = round(3 * d) # 1
N6 = round(6 * d) # 2

class MoE_Bottleneck(nn.Module):
    """Mixture of Experts Bottleneck module."""
    def __init__(self, c1, c2, shortcut=True):
        super().__init__()
        self.c1 = c1
        self.c2 = c2
        self.shortcut = shortcut
        c_ = int(c2 * 0.5)

        # 1. Standard Convs
        self.conv1 = Conv(c1, c_, 1, 1)
        self.conv3 = Conv(c_, c2, 1, 1)

        # 2. MoE Layer (Replaces the second 3x3 Conv)
        self.router = nn.Conv2d(c_, E, kernel_size=1)
        self.experts = nn.ModuleList([
            Conv(c_, c_, 3, 1) for _ in range(E)
        ])

    def forward(self, x):
        x_in = self.conv1(x)
        B, C, H, W = x_in.shape
        x_reshaped = x_in.permute(0, 2, 3, 1).reshape(-1, C)

        # Gating and Top-K Selection
        router_logits = self.router(x_in).permute(0, 2, 3, 1).reshape(-1, E)
        routing_weights = F.softmax(router_logits, dim=-1)
        top_k_weights, top_k_indices = torch.topk(routing_weights, K, dim=-1)
        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)

        # Sparse Execution
        output_moe = torch.zeros_like(x_reshaped)

        for i, expert in enumerate(self.experts):
            expert_mask = (top_k_indices == i).any(dim=-1)
            selected_indices = torch.nonzero(expert_mask).squeeze(1)

            if selected_indices.numel() == 0:
                continue

            expert_input = x_in[:, :, selected_indices // W % H, selected_indices % W].reshape(-1, C, 1, 1)
            expert_output = expert(expert_input).squeeze(-1).squeeze(-1)

            expert_weight_idx = (top_k_indices[selected_indices] == i).nonzero(as_tuple=True)[1]
            expert_weights = top_k_weights[selected_indices, expert_weight_idx]

            output_moe[selected_indices] += expert_output * expert_weights.unsqueeze(-1)

        output_moe = output_moe.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()
        x_out = self.conv3(output_moe)

        return x_out + x if self.shortcut and x.shape[1] == x_out.shape[1] else x_out

class C2f_MoE(C2f):
    """C2f module with MoE Bottleneck."""
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)

        # Replace the dense Bottleneck module list with the Sparse MoE Bottleneck
        self.m = nn.ModuleList([
            MoE_Bottleneck(c_, c_, shortcut) for _ in range(n)
        ])

class YOLOv8n_MoE_C2f(nn.Module):
    """YOLOv8n with Mixture of Experts in the head."""
    def __init__(self, nc=nc):
        super().__init__()
        self.nc = nc

        # --- Backbone (Standard C2f) ---
        self.b_0 = Conv(3, C1, 3, 2)
        self.b_1 = Conv(C1, C2, 3, 2)
        self.b_2 = C2f(C2, C2, n=N3, shortcut=True)
        self.b_3 = Conv(C2, C3, 3, 2)
        self.b_4 = C2f(C3, C3, n=N6, shortcut=True)
        self.b_5 = Conv(C3, C4, 3, 2)
        self.b_6 = C2f(C4, C4, n=N6, shortcut=True)
        self.b_7 = Conv(C4, C5, 3, 2)
        self.b_8 = C2f(C5, C5, n=N3, shortcut=True)
        self.b_9 = SPPF(C5, C5, k=5)

        # --- Head Layers (MoE C2f) ---
        self.h_10 = Upsample(scale_factor=2, mode='nearest')
        self.h_12 = C2f_MoE(C5 + C4, C4, n=N3, shortcut=False)  # P4 Aggregation (MoE)

        self.h_13 = Upsample(scale_factor=2, mode='nearest')
        self.h_15 = C2f_MoE(C4 + C3, C3, n=N3, shortcut=False)  # P3 Output (MoE)

        self.h_16 = Conv(C3, C3, 3, 2)
        self.h_18 = C2f_MoE(C3 + C4, C4, n=N3, shortcut=False)  # P4 Output (MoE)

        self.h_19 = Conv(C4, C4, 3, 2)
        self.h_21 = C2f_MoE(C4 + C5, C5, n=N3, shortcut=False)  # P5 Output (MoE)

        self.h_22 = Detect(nc=self.nc, ch=[C3, C4, C5])

    def forward(self, x):
        cache = []
        x = self.b_0(x); x = self.b_1(x); x = self.b_2(x)
        x_3 = self.b_3(x); cache.append(x_3)  # P3/8
        x = self.b_4(x_3)
        x_5 = self.b_5(x); cache.append(x_5)  # P4/16
        x_6 = self.b_6(x_5)
        x = self.b_7(x_6); x = self.b_8(x)
        x_9 = self.b_9(x)  # P5/32

        # Head (MoE)
        x_p4_up = self.h_10(x_9)
        x_12 = self.h_12(torch.cat((x_p4_up, x_6), 1))

        x_p3_up = self.h_13(x_12)
        x_15 = self.h_15(torch.cat((x_p3_up, x_3), 1))

        x_p3_down = self.h_16(x_15)
        x_18 = self.h_18(torch.cat((x_p3_down, x_12), 1))

        x_p4_down = self.h_19(x_18)
        x_21 = self.h_21(torch.cat((x_p4_down, x_9), 1))

        return self.h_22([x_15, x_18, x_21])

def load_pretrained_weights(custom_moe_model):
    """Load pretrained YOLOv8n weights into the MoE model."""
    print("Downloading official YOLOv8n weights...")
    official_yolo = YOLO('yolov8n.pt')
    pretrained_state_dict = official_yolo.model.state_dict()

    KEY_MAPPING = {
        0: 'b_0', 1: 'b_1', 2: 'b_2', 3: 'b_3', 4: 'b_4',
        5: 'b_5', 6: 'b_6', 7: 'b_7', 8: 'b_8', 9: 'b_9',  # Backbone End
        10: 'h_10', 13: 'h_13', 16: 'h_16', 19: 'h_19'  # Standard Convs/Upsample in Head
    }

    new_state_dict = {}

    for key, value in pretrained_state_dict.items():
        try:
            parts = key.split('.')
            layer_index = int(parts[1])
            param_path = '.'.join(parts[2:])
        except (IndexError, ValueError):
            continue

        if layer_index <= 21:
            if layer_index in KEY_MAPPING:
                custom_attr = KEY_MAPPING[layer_index]
                if 'm.' not in param_path:
                    new_key = f'{custom_attr}.{param_path}'
                    new_state_dict[new_key] = value

    incompatible_keys = custom_moe_model.load_state_dict(new_state_dict, strict=False)

    print(f"âœ… Loaded {len(new_state_dict)} tensors from YOLOv8n weights.")
    print(f"âš ï¸ Missing MoE/Detect layer weights: {len(incompatible_keys.missing_keys)}.")

    return custom_moe_model

def get_device():
    """Automatically detect and return the best available device."""
    if torch.cuda.is_available():
        device = torch.device('cuda:0')
        device_name = f"CUDA ({torch.cuda.get_device_name(0)})"
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        device_name = "MPS (Apple Silicon)"
    else:
        device = torch.device('cpu')
        device_name = "CPU"
    
    print(f"ðŸ”§ Detected device: {device_name}")
    return device

def main():
    """Main training function using Ultralytics YOLO framework."""
    
    # Check if dataset YAML exists
    if not os.path.exists(DATA_YAML_PATH):
        print(f"âŒ Error: Dataset configuration not found at {DATA_YAML_PATH}")
        print("Please run 'python prepare_dataset.py' first to download and prepare the dataset.")
        return

    print("\n" + "="*50)
    print("     USING ULTRALYTICS YOLO TRAINER (Simplified)")
    print("="*50)
    print("\nâ„¹ï¸ Note: Training YOLOv8n-MoE with Ultralytics requires")
    print("   direct model integration or custom training loop.")
    print("   Using simplified approach with pre-trained YOLOv8n\n")
    
    # For now, let's just train a standard YOLOv8n model
    # and document that the MoE architecture needs a custom training loop
    
    print("Training standard YOLOv8n as baseline...")
    print("(Full MoE integration requires custom training loop - see documentation)\n")
    
    # Auto-detect device
    device = get_device()
    device_str = 'mps' if device.type == 'mps' else (0 if device.type == 'cuda' else 'cpu')
    
    # Train standard YOLOv8n
    model = YOLO('yolov8n.pt')
    
    results = model.train(
        data=DATA_YAML_PATH,
        epochs=EPOCHS,
        batch=BATCH_SIZE,
        imgsz=IMG_SIZE,
        name='yolov8n_baseline_pet',
        device=device_str,
        patience=20,
        save=True,
        plots=True
    )

    print("\n" + "="*50)
    print(f"âœ… Baseline training completed!")
    print(f"Results saved to: runs/detect/yolov8n_baseline_pet")
    print("\nâš ï¸ For MoE training, a custom training loop is required.")
    print("   See train_moe_custom.py for the custom implementation.")
    print("="*50)

if __name__ == "__main__":
    main()
